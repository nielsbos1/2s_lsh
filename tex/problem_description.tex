\section{Problem Description}
Since the invention and introduction of the World Wide Web in 1989, the number of available webpages and the amount of information that is contained on those pages has been growing exponentially. In the past decade, commercial use of the internet has flourished as well: online shopping has never been as big as today and has replaced physical stores as the main method of obtaining consumer products for most people. One of the advantages of online shopping for consumers is that they can look for the best price of a product from virtually anywhere. To further enhance the experience of the customer, websites have popped up that aggregate product listing data from other websites and point customers to the best deal out there. 
This process of aggregating product listing data does come with a few challenges. An important step in the aggregation of a set of elements is deciding whether two elements are actually the same. When we deal with structured data, such as elements that are stored in relational databases, it is quite straightforward to come up with a comparison method. In comparing two rows, one simply takes the elements of the two rows in the same column to decide whether the rows refer to the same object. 
Since there are no universal rules governing the internet, products are listed in different ways on different websites. This results in data on products generally being unstructured. While some websites choose to incorporate many technical details on the product in the product title, there are other websites that choose to just use the name of the manufacturer and the product identifier. Moreover, websites are generally free to choose in what way and to what extent other characteristics of the product are listed on the website. In the case of products with a screen, such as smartphones, laptops and televisions, most websites will list the size of the screen, but it might be that some websites do not incorporate information on some more irrelevant elements, such as the size of the bezel. These differences in the representation of information make it hard for comparison websites to decide whether representations of two products from different websites actually refer to the same product and thus can be regarded as duplicates. 
The problem described above is related to the field of similarity estimation. Within this field algorithms are being developed that aim to estimate the similarity between inputs. For this specific application several algorithms have been proposed.
The Title-Model-Worlds-Method (TMWM) extracts and compares so-called 'model words' from the listed product title to estimate the similarity between products. 'model words' are words that are made up of a numeric and non-numeric part, such as '60hz or '5"'. The Hybrid-Similarity-Method (HSM) extends on this work by also taking into account the key-value pairs in the product listing. For televisions such a key-value pair could for example be 'Screen size: 60 inch'. Combining some of the ideas behind these previous mentioned methods, the Multi-component Similarity Method (MSM) introduces an agglomerative hierarchical clustering algorithm that improves vastly on the aforementioned methods. One issue that is being dealt with is the scalability of the method as the number of products grows. By using Locality Sensitive Hashing (LSH), the number of comparisons can be greatly decreased, while still performing well. LSH achieves this by creating a pre-selection of candidate pairs to which the MSM is applied. This does imply that all actual duplicates that are regarded as a candidate pair by the LSH algorithm will not be checked by the MSM and thus not be recognized as such.
The challenge that remains is twofold:  On one hand it is important that the pair quality is high, so that we do not make computationally costly, unnecessary comparisons. On the other hand we wish a high pair completeness, so that we do not miss out on a lot of similar pairs. We need to achieve both things, while simultaneously keeping the computational cost of the hashing algorithm in check, so that scalability is guaranteed.