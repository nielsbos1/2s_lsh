The base method on which further extensions/ideas will be developed is the Multi-component Similarity Method with Preselection+ (MSMP+). The method achieves improvements with respect to scalability and computational requirements compared to earlier proposed methods such as the Title Model Words Method (TMWM), the Hybrid Similarity Method and the regular Multi-component Similarity Method. This has mainly been the result of the introduction of Locality-Sensitive Hashing (LSH), which is a hashing scheme that generates candidate pairs for comparison by hashing objects that are likely to be similar into the same bucket. The MSMP+ algorithm is then only applied to those candidates that reside in the same bucket.
The current implementation of LSH can be improved in two ways: On one hand we would like to generate candidate pairs that are on average of better quality, on the other hand we would like to need less computations to come to these candidate pairs. By achieving these two goals, the MSMP+ method will be better applicable to tasks with a larger scale of unstructured data.
There exists several families of so-called Locality-Sensitive Hashing functions. The one that is used in the current implementations of MSMP+ is min-hashing. This hashing function produces hashes such that the probability that resulting hashes of two different sets of elements are the same equals the Jaccard similarity of the two sets. Several attempts have been made in the literature to decrease the computational costs of min-hashing, since the method in its original form requires the creation of many random permutations of the rows of the signature matrix. A method that seems to generate positive results in this context is densified one-permutation min-hashing. Densified one-permutation min-hashing promises computational advantages over traditional forms of min-hashing, since it requires only one permutation to generate the hash signatures of input vectors. It has been empirically shown that for some well-known similarity problem this method performs on par with traditional methods of min-hashing. There is still the open question whether this performance is maintained when applied to the problem of product deduplication.
Besides considering alternative implementations of the min-hashing method, we can also opt for hashing functions that belong to one of the other families of the LSH functions. The main difference between families is related to the choice of approximated similarity function of hashing functions belonging to the family. In the case of the sign normal random projection (simhash), the chance of collision between two hash functions approximates the cosine similarity of the input vectors. For hashing functions in the family based on p-stable distributions the change of collision approximates the Euclidean distance between the input vectors.  

Now we come to the question of how to compare these different LSH variants. We would be interested in theoretical and empirical estimates of their scalability with respect to the problem in place: As the size of input grows, to what extent do the computational costs increase as well? Next to that, it is important to determine to what extent other LSH variants deliver candidate pairs of good quality to apply the  MSMP+ method to, but also whether the LSH variants do not miss out on too much candidate pairs, thereby causing false negatives. The former is measured by the pair quality, the number of duplicates found as a fraction of the total number of comparisons made. The latter is measured by the pair completeness, the number of duplicates found as a fraction of the total number of duplicates. Ideally we would like both measures to be as high as possible (close to 1), but in practice we observe a trade-off. Hence, in order to judge the overall performance of a LSH function in one metric, we construct the F1* measure, the harmonic mean of the pair quality and pair completeness. Furthermore, it is of interest to find out what happens with respect to the aforementioned measures when we decrease or increase the overall number of comparisons that we generate. This can be done by varying the threshold t, which represents the relation between the number of false positives and false negatives. A lower threshold leads to more comparisons and usually more false positives, while a higher threshold is associated with more false negatives. 
In order to properly compare the proposed LSH variants, it is not appropriate to apply the different methods to just one set of the test sample. Especially if we would like further insight into the stability of the methods, it is worthwhile to apply a bootstrapping method to generate multiple test samples. We apply the LSH variants to all samples and compute the metrics mentioned in the previous section for each set of input parameters.
After gaining further insights into the performance of functions in the different families on several aspects, we could start looking into combining members of the same or different family with logical AND/OR constructions to achieve better performance. In case of an AND construction, we lower the probability of being declared similar for arbitrary vectors x and y. The OR construction on the other hand raises this probability. By combining these two constructions iteratively, we can construct a LSH family that assigns more similar input vectors a relatively higher similarity probability, while assigning less similar input vectors a relatively lower similarity probability. This could lead to the decrease of both the false positive and false negative rate. The application of these constructions does come at the cost of higher computation requirements.