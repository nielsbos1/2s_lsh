\chapter{Conclusion}
\label{chap:conclusion}
The primary objective of this research is to gain a better understanding of how the application of LSH in the MSMP+ algorithm can be improved. The MSMP+ algorithm is a method proposed by \cite{HartveldKMNPFS18} that can be used for product deduplication, which is the task of deciding which products listed in different Web shops relate to the same real-world entity. LSH has been shown to be a suitable method for producing a set of comparisons of greatly reduced size that still contains a large number of the duplicates. Before applying LSH, we extract so-called model words from the title and descriptions of products. Using a hash function, the model words are then mapped to an integer-based representation. These sets of integers are then used as input for the LSH scheme. In this research, we evaluate two research directions that might improve the results of LSH in this application. 

The first direction is the evaluation of an alternative LSH scheme, FSS. \cite{DahlgaardKT17}, who created FSS, claim that the Jaccard similarity estimates are more precise than the traditionally used MinHash scheme. We can empirically confirm that the sketches resulting from FSS result in more precise estimates of the Jaccard similarity than those created by the MinHash scheme. This research adds to the existing research by being the first to implement FSS in a LSH scheme and showing that its effectiveness is on par with the MinHash sketching function. Since FSS is also a more efficient sketching scheme for all practical LSH applications, we argue that FSS is the superior choice for LSH Jaccard similarity schemes.

The second direction is the main contribution of this work. It is related to an improved understanding of ampification of LSH schemes, building a solid foundation for future research to do further experiments. We can separate this understanding in several layers. 

First, this research defines a parametrized framework that aids the understanding of amplified LSH schemes. The parametrized framework also allows for non-amplified configurations, showing that the amplified LSH scheme can be regarded as a generalization to non-amplified LSH schemes. Using the new parametrization with parameters ($r_1,b_1,r_2,b_2$), we are able to define the amplified version of the s-curve. We show that there is no analytical solution available to the optimization of the s-curve with respect to the threshold $t$.

Second, we work out necessary algorithms for the application of amplified LSH schemes. The first one is an optimization algorithm for amplified schemes, which aims to minimize $z(\cdot)$, the weighted average of $Pr[\text{False negative}]$ and $Pr[\text{False positive}]$. Next to that, we devise an efficient algorithm for extracting candidate pairs. Both algorithms have been integrated in the LSHAmplified class, extending the Python \textit{datasketch} package, adding an implementation of amplified LSH schemes that is ready-to-use out of the box. 

Third, we use the optimization algorithm mentioned above to find the optimal set of parameter values ($r_1^{opt},b_1^{opt},r_2^{opt},b_2^{opt}$) for a wide range of thresholds $t$ and number of hash functions $n$.Additionally, for each set of values, we calculate the relative decrease in $\hat{z}$, the optimized value of $z(\cdot)$, as a result of amplification. We find that higher values of $n$ are associated with larger benefits of amplification. Moreover, this research indicates that schemes with smaller threshold $t$ stand to benefit more from amplification than those with a larger threshold. Here we should note that these benefits come at a cost: optimized amplified schemes for small threshold values of $t$ have a small value of $r_1$, which increases the costs of extracting candidate pairs, as we also show experimentally.

Fourth, we present experimental bootstrapped results of our method, where we aim to compare amplified schemes with non-amplified schemes. Amplified schemes indeed come through on their theoretical promises: keeping $n$ constant, they are able to return the same number of actual duplicates, while returning less candidate comparisons in total. If the hash functions used in a LSH scheme are relatively costly or storage is limited, then amplification could be an appropriate method for reducing the number of underlying hash functions $n$, while keeping up the performance.

We see several interesting directions for future research. The methods used in our current implementation, MinHash and FSS, consider all elements equally, giving every model word the same weight. There might be a case to be made that depending on the specific attribute that a model word is extracted from, a higher or lower weight should be added to that model word. For an extensive review of such methods, we would like to direct the reader to work by \cite{WuLCGZ22}.

Next to that, we want to place emphasis on the specific characteristics of our dataset. Compared to other LSH applications, the actual Jaccard similarity of non-duplicates and duplicates is on the lower side. This means that LSH schemes optimized for relatively high thresholds have a bad performance on our dataset. In order to properly prove the general applicability of the amplified LSH scheme, it would be interesting to conduct experiments on data sets with higher average Jaccard similarity as well.  

Lastly, we would recommend future researchers to perform a deeper analysis of computation costs. On the one hand, this research finds that amplified LSH schemes are as effective as non-amplifed LSH schemes, but require a lower number of hash functions $n$, which decreases computational costs. On the other hand, amplified schemes are associated with amaller hash tables, which increases computational costs of extracting candidate pairs. A deeper analysis that further quantifies these costs, while relating them to the threshold $t$, would allow users to always configure LSH as best fits their needs. 


%Look at trade-off between scalability at smaller threshold and performance. Depending on application, varying with $W_fp$ and $W_fn$ also interesting

% Mentioned that t is a way to tune LSH scheme 
% bliksemschichtje in plots