\chapter{Introduction}\label{chap:introduction}
In this chapter we introduce the subject of this thesis. In the first subsection we discuss the problem at hand. Then we move on to the research objectives. Lastly we lay down the structure of the remainder of this thesis. 
\section{Problem Statement}
\label{sect:problem_statement}
Since the invention and introduction of the World Wide Web in 1989, the number of available webpages and the amount of information that is available on those pages has been growing exponentially. In the past decade, commercial use of the Internet has flourished as well: online shopping has never been as big as today and has replaced shopping in physical stores as the main method of obtaining consumer products for most people. One of the advantages of online shopping for consumers is that they can look for the best price of a product from virtually anywhere. To support the customer in product selection, websites have popped up that aggregate product listing data from other websites and point customers to the best deal out there. 

This process of aggregating product listing data does come with a few challenges. An important step in the aggregation of a set of entities is deciding whether two entities from different sources are actually the same product. When we deal with structured data, such as entities that are stored in relational databases, it is quite straightforward to come up with a comparison method. In comparing two rows in a relational database, we can do a comparison on a column-per-column basis to decide whether the rows refer to the same object. 

Such a straightforward approach is unfortunately not available for the problem of interest in this thesis. Since there are no universal rules governing the Internet, products are listed in different ways on distinct websites. This results in data on products generally being unstructured. While some websites choose to incorporate many technical details on the product in the product title, there are other websites that choose to just show key details such as the name of the manufacturer and the product identifier. Moreover, websites are generally free to choose in what way and to what extent other characteristics of the product are listed on the website. In the case of products with a screen, such as smartphones, laptops, and televisions, most websites  list the size of the screen, but it is often the case that websites do not incorporate information on some less relevant attributes, such as the size of the bezel. These differences in the representation of information make it hard for comparison websites to decide whether representations of two products from different websites actually refer to the same real-world entity and can thus be regarded as duplicates.

The problem described in the previous paragraph is related to the field of similarity estimation. Within this field algorithms are  developed that aim to estimate the similarity between inputs. For this specific application several algorithms have been proposed, as discussed further below.

The \textit{multi-component similarity method with pre-selection+} (MSMP+),  proposed by \cite{HartveldKMNPFS18}, is such an algorithm and will form the starting point from where further research directions in this thesis will be developed. The key ideas of MSMP+ are borrowed from earlier work by \cite{DamGKNVF16}, who introduced the \textit{multi-component similarity method with pre-selection} (MSMP). The MSMP algorithm actually consists of multiple building blocks, that have been shown over time to work well together. From earlier work by \cite{BezuBRVVF15}, \cite{DamGKNVF16} borrow the \textit{multi-component similarity method} (MSM), an adapted single-linkage hierarchical clustering method. MSM outperforms other state-of-the-art methods at the task of product deduplication, but comes at the disadvantage of being computationally costly. That's why \cite{DamGKNVF16} add a preprocessing step to the algorithm that consist of a \textit{locality-sensitive hashing} (LSH) scheme, which is a hashing method that creates buckets of items that are (very) likely to be similar. In their implementation they extract so-called \textit{model words} from the title and create binary vectors for each product in the dataset, where a $1$ denotes the occurence of a \textit{model word} in the title and a $0$ refers to the absence of the \textit{model word}. From these binary vectors hashing functions are used to generate a sequence of integers, which are often called sketches. We now arrive at the key idea behind the LSH algorithm: the chance that individual elements of the sketches are equal for two inputs corresponds to the actual similarity between the two inputs. Sketches are then split in smaller parts that we call buckets. The greater the similarity between the inputs, the higher the chance that those inputs are hashed to the same bucket. Inputs that are hashed to the same bucket are called candidate pairs. Thus, in this case MSM is only applied to the comparisons found in the buckets, greatly reducing the number of computations necessary, which in turn lowers the computational cost of the complete algorithm. As long as LSH is applied in such a way that most actual duplicates are contained in the same bucket at least once, MSMP delivers a good accuracy. 

\cite{HartveldKMNPFS18} further improve the LSH step by also including model words from key-value pairs in the description of products as input for the binary vectors. Other accuracy improvements are achieved by adding a data cleaning step that boosts the data quality. In both implementations a \textit{min-wise independent permutations locality-sensitive hashing} (MinHash) scheme is used, which hashes items to the same bucket with a probability that is equal to the Jaccard similarity between the respective items. By tuning the parameters of the LSH scheme, we can guarantee that the majority of actual duplicates will be among the candidate pairs, which form a fraction of the total number of pairs. Further improvements to the LSH scheme in place should be focussed on four important metrics. On one hand we would like to see an increase of the \textit{pair quality} (PQ) among the candidate pairs, so that they will contain more duplicates in less candidate pairs. On the other hand it would be of importance that the \textit{pair completeness} (PC) increases, which means that a greater fraction of the total number of duplicates will be among the proposed candidate pairs. Both metrics are indirectly related to the third metric. That is the \textit{fraction of comparisons} (FoC), which is defined as the number of candidate pairs resulting from the LSH scheme divided by the total number of possible comparisons in the dataset. A lower number of candidate pairs is generally associated with a higher value of PQ, but might hamper PC. Moreover, a low value of the FoC metric implies low computational costs related to the MSM step. The information in the FoC metric is sometimes expressed in the form of the \textit{reduction ratio} (RR), which is defined as $1 - \text{FoC}$. This metric measures the degree to which the number of comparisons to be made has been lowered as a result of the LSH scheme. Next to these metrics, we are also mindful of the (time) efficiency of the LSH scheme, so that an application to very large datasets is feasible.
\section{Research Objectives}
\label{sect:research_objectives}

This research focusses on optimizing the LSH step in the MSMP+ algorithm. There are essentially two stages in the LSH step where we can look for innovative changes. In the first stage, sketches of the inputs are created using a so-called sketching scheme. Since the first introduction of the LSH scheme by \cite{IndykM98}, many sketching variations have been proposed. Currently, the MSMP+ algorithm uses the MinHash sketching scheme, put forward by \cite{Broder00}. This scheme is designed to identify duplicates based on the Jaccard similarity, which is a  measure for estimating the similarity between sets. Since then, other Jaccard similarity sketching schemes have been proposed that promise efficiency benefits, such as \textit{one permutation hashing} (OPH) \citep{Ping2012}, or effectiveness benefits, such as \textit{fast similarity sketching} (FSS), a more recent effort by \cite{DahlgaardKT17}.  

Next to changing the method of creating sketches, we can also look at changes to the second stage. In this stage the sketch is split in parts and hashed to buckets, from which candidate pairs are extracted. Much less attention has been paid to this part of the LSH algorithm than to the sketching methods. In their comprehensive review of LSH, \cite{LeskovecRU14} describe the concept of amplification. This entails using AND/OR constructions applied to LSH functions, allowing for more complex LSH structures. While the idea sounds very promising in theory, there has been no theoretical nor experimental evaluation.

The main goal of this research is to optimize the LSH step in the MSMP+ algorithm. We do this by experimenting with the variations on LSH as described in the previous paragraphs. This leads us to the following main research question:
\begin{quote}
	\emph{How to optimally select candidate pairs for product deduplication using LSH in the MSMP+ algorithm?}
\end{quote}
Following the discussion in the previous paragraphs, we have decided to formulate multiple subquestions, as listed below.
\begin{quote}
	\begin{enumerate}
	\item \emph{How do more recently proposed schemes related to the Jaccard similarity perform compared to the original MinHash implementation?}
    \item \emph{How do we optimally combine hashing functions using AND/OR constructions into new LSH families?}
	\end{enumerate}
\end{quote}

\section{Thesis Structure}
\label{sect:structure}
In the paragraphs above, we have introduced the focus of this research, with the specific research questions. In Chapter \ref{chap:lit_review} we offer a comprehensive literature review. The chapter starts with a general introduction to deduplication methods, after which we direct our focus on LSH.  Several variations of sketching functions are discussed and compared, after which we end the chapter with an introduction to the idea of amplification of LSH families. In Chapter \ref{chap:data}, we proceed with an introduction to the data, while also explaining which transformations are applied to the data to prepare the data for the methods. Chapter \ref{chap:methodology} contains an extensive discussion of the methodology. Besides describing the details of implementation of the sketching schemes tested, we also devise a new parametrized framework for amplified LSH schemes. With this framework in hand we lay out complete instructions for the implementation of an amplified LSH scheme, presenting detailed algorithms where appropriate. The Chapter ends with a discussion of the evaluation methodology. The results can be found in Chapter \ref{chap:results}. We provide great insight into the theoretical optimal configuration of amplified LSH schemes for different hyperparameter settings. We additionally present experimental bootstrapped results, comparing MinHash with FSS, and amplified LSH schemes with non-amplified LSH schemes. Last but not least, the thesis comes to an end in Chapter \ref{chap:conclusion}, where we not only provide concluding remarks, but also shine our light on limitations of the research and interesting future research directions.  
